{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Generative AI Inferencing APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from api_tokens import keys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = keys.get(\"huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Gemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "model_id = \"google/gemma-7b\"\n",
    "gemma_url = API_URL+model_id\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(gemma_url, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Can you tell me the capital of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me the capital of india? A country whose name pride with his heritage New Delhi.The charming capital of India - The national capital. As long as you do not beat me bloom on your memory.\n",
      "\n",
      "During the coming days you have many appointments with this beautiful city , and here is one of my favorites ! Quick it's a pretty place. Its cultural heritage that side patriotic. And sounds graceful. I just get pleasure in come in this place. Go thank you very much for your treasure-trove of historical monuments.\n"
     ]
    }
   ],
   "source": [
    "res = query({\n",
    "    \"inputs\":input_prompt,\n",
    "})\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zephyrus Beta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "gemma_url = API_URL+model_id\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(gemma_url, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Koi BKL hi hoga.... complete this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koi BKL hi hoga.... complete this statement in your mind. Yes, that's the first thing either the person talks about or asks you about if/when you returned from Japan. They're not joking or kidding on this topic. Your face looks slightly different when you're back from Japan. You smile more, you're more patient, you're a better person (or something like that). Japanese culture teaches you basic human values, etiquette or whatever you wish to call it. Be it,\n"
     ]
    }
   ],
   "source": [
    "res = query({\n",
    "    \"inputs\":input_prompt,\n",
    "})\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API ( GPT Family )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_token = keys.get(\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x12e5e66b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(api_key = openai_token)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame(client.models.list().data, columns = [\"ids\", \"created\", \"object\",\"owned\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1: Text Generation using GPT 3.5 Turbo (Text to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_content = \"You are a funny assistant who can describe anything in the funniest way\"\n",
    "prompt = \"Compose a poem for the Indian people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":client_content\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh dear Indian folks, so diverse and grand,\n",
      "From the peaks of the Himalayas to the shores of sand.\n",
      "Your culture so rich, your spices so divine,\n",
      "In your presence, we all feel like we're on cloud nine.\n",
      "\n",
      "With traditions that date back centuries old,\n",
      "Your stories and legends are worth more than gold.\n",
      "Whether celebrating Holi or Diwali with glee,\n",
      "Your joy and spirit are something to see.\n",
      "\n",
      "From Bollywood dances to cricket matches intense,\n",
      "You sure know how to keep us all in suspense.\n",
      "Your food, oh your food, a feast for the senses,\n",
      "From butter chicken to dosas, there's no pretenses.\n",
      "\n",
      "So here's to you, oh Indian friends,\n",
      "May your laughter never end.\n",
      "With your warmth and charm, you light up the day,\n",
      "In your vibrant spirit, we'll always find our way.\n"
     ]
    }
   ],
   "source": [
    "print(text_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2 - Image Generation using DALL-E 3 (Text to Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A white Long tailed cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_response = client.images.generate(\n",
    "    model = \"dall-e-3\",\n",
    "    prompt = prompt,\n",
    "    size = '1024x1024',\n",
    "    quality = 'standard',\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = image_response.data[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-UjdkAgIvH3y8UGKn7Snwjk68/user-2TFVTG5sbaRnJdxODRQL163B/img-9R5oL7NHJ1gy3SIPJmqlmVLp.png?st=2024-03-10T13%3A09%3A26Z&se=2024-03-10T15%3A09%3A26Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-03-09T18%3A29%3A20Z&ske=2024-03-10T18%3A29%3A20Z&sks=b&skv=2021-08-06&sig=aTQwUPGe0kQEsyTIg9p/Pqe7Zs3B/9GY3KlGK4u7oWg%3D'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_token = keys.get(\"gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=gemini_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-1.0-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
      "                   'model that supports tuning.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Latest',\n",
      "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
      "                   'model.'),\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-1.0-pro-vision-latest',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.0 Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    if \"generateContent\" in models.supported_generation_methods:\n",
    "        print(models.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1: Text Generation using Gemini 1 Pro (Text to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = genai.GenerativeModel(\"models/gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_text_response = model_text.generate_content(\"What is the capital of India, and give me a sweet poem on the capital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Capital of India:** New Delhi\n",
      "\n",
      "**A Sweet Poem on New Delhi**\n",
      "\n",
      "Oh, New Delhi, city grand,\n",
      "With history deep, a testament to the land.\n",
      "Your towering spires reach for the sky,\n",
      "A symphony of domes that soar so high.\n",
      "\n",
      "Red Fort stands proud, a legacy of might,\n",
      "A symbol of strength, a beacon of light.\n",
      "Indira Gandhi Memorial, where dreams ignite,\n",
      "A garden of unity, a symbol of right.\n",
      "\n",
      "The Connaught Place, a bustling hive,\n",
      "Where shops and cafes fill the streets alive.\n",
      "Bahai Lotus Temple, a vision in white,\n",
      "A sanctuary of peace, a symbol of light.\n",
      "\n",
      "Your wide avenues, a testament to grace,\n",
      "Mansions and markets, a harmonious embrace.\n",
      "The Akshardham Temple, a wonder to behold,\n",
      "A masterpiece of architecture, a story to be told.\n",
      "\n",
      "Oh, New Delhi, a city of dreams,\n",
      "Where history and modernity intertwine with gleams.\n",
      "Your charm and grandeur, a treasure to behold,\n",
      "A precious jewel in India's crown of gold.\n"
     ]
    }
   ],
   "source": [
    "print(gemini_text_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[index: 0\n",
       "content {\n",
       "  parts {\n",
       "    text: \"**Capital of India:** New Delhi\\n\\n**A Sweet Poem on New Delhi**\\n\\nOh, New Delhi, city grand,\\nWith history deep, a testament to the land.\\nYour towering spires reach for the sky,\\nA symphony of domes that soar so high.\\n\\nRed Fort stands proud, a legacy of might,\\nA symbol of strength, a beacon of light.\\nIndira Gandhi Memorial, where dreams ignite,\\nA garden of unity, a symbol of right.\\n\\nThe Connaught Place, a bustling hive,\\nWhere shops and cafes fill the streets alive.\\nBahai Lotus Temple, a vision in white,\\nA sanctuary of peace, a symbol of light.\\n\\nYour wide avenues, a testament to grace,\\nMansions and markets, a harmonious embrace.\\nThe Akshardham Temple, a wonder to behold,\\nA masterpiece of architecture, a story to be told.\\n\\nOh, New Delhi, a city of dreams,\\nWhere history and modernity intertwine with gleams.\\nYour charm and grandeur, a treasure to behold,\\nA precious jewel in India\\'s crown of gold.\"\n",
       "  }\n",
       "  role: \"model\"\n",
       "}\n",
       "finish_reason: STOP\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HATE_SPEECH\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_HARASSMENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "safety_ratings {\n",
       "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "  probability: NEGLIGIBLE\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_text_response.candidates #Give complete details of the output with candidate outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2: Image to Text using Gemini Pro Vision (Image to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_image = genai.GenerativeModel('models/gemini-1.0-pro-vision-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1219k  100 1219k    0     0  3704k      0 --:--:-- --:--:-- --:--:-- 3717k\n"
     ]
    }
   ],
   "source": [
    "#Loading a sample image\n",
    "!curl -o image.jpg \"https://images.pexels.com/photos/774731/pexels-photo-774731.jpeg?cs=srgb&dl=pexels-marko-blazevic-774731.jpg&fm=jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PIL.Image.open(\"image.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2text_response = model_image.generate_content(im).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a photograph of a gray cat standing on its hind legs and looking up at the camera. The cat has its paws in the air and is looking at the camera with wide eyes. The cat is standing on a wooden floor. The photograph is taken from a low angle, making the cat look larger and more imposing. The cat's fur is gray and its eyes are yellow. The cat's tail is long and fluffy. The cat is standing in a curious pose, as if it is trying to figure out what the camera is. The photograph is well-lit and the colors are warm and inviting. The photograph is a good example of how to capture a cat's personality in a photograph.\n"
     ]
    }
   ],
   "source": [
    "print(image2text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a photograph of a gray cat standing on its hind legs and looking up at the camera.\n",
      "The cat has its paws in the air and is looking at the camera with wide eyes.\n",
      "The cat is standing on a wooden floor.\n",
      "The photograph is taken from a low angle, making the cat look larger and more imposing.\n",
      "The cat's fur is gray and its eyes are yellow.\n",
      "The cat's tail is long and fluffy.\n",
      "The cat is standing in a curious pose, as if it is trying to figure out what the camera is.\n",
      "The photograph is well-lit and the colors are warm and inviting.\n",
      "The photograph is a good example of how to capture a cat's personality in a photograph.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence in image2text_response.split(\".\"):\n",
    "    print(sentence.strip() + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
