{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Generative AI Inferencing APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from api_tokens import keys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = keys.get(\"huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Gemma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "model_id = \"google/gemma-7b\"\n",
    "gemma_url = API_URL+model_id\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(gemma_url, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Can you tell me the capital of india?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you tell me the capital of india? A country whose name pride with his heritage New Delhi.The charming capital of India - The national capital. As long as you do not beat me bloom on your memory.\n",
      "\n",
      "During the coming days you have many appointments with this beautiful city , and here is one of my favorites ! Quick it's a pretty place. Its cultural heritage that side patriotic. And sounds graceful. I just get pleasure in come in this place. Go thank you very much for your treasure-trove of historical monuments.\n"
     ]
    }
   ],
   "source": [
    "res = query({\n",
    "    \"inputs\":input_prompt,\n",
    "})\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zephyrus Beta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"https://api-inference.huggingface.co/models/\"\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "gemma_url = API_URL+model_id\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(gemma_url, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"Koi BKL hi hoga.... complete this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koi BKL hi hoga.... complete this statement in your mind. Yes, that's the first thing either the person talks about or asks you about if/when you returned from Japan. They're not joking or kidding on this topic. Your face looks slightly different when you're back from Japan. You smile more, you're more patient, you're a better person (or something like that). Japanese culture teaches you basic human values, etiquette or whatever you wish to call it. Be it,\n"
     ]
    }
   ],
   "source": [
    "res = query({\n",
    "    \"inputs\":input_prompt,\n",
    "})\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI API ( GPT Family )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_token = keys.get(\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x12e5e66b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(api_key = openai_token)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models = pd.DataFrame(client.models.list().data, columns = [\"ids\", \"created\", \"object\",\"owned\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1: Text Generation using GPT 3.5 Turbo (Text to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_content = \"You are a funny assistant who can describe anything in the funniest way\"\n",
    "prompt = \"Compose a poem for the Indian people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_response = client.chat.completions.create(\n",
    "    model = \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":client_content\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\":prompt\n",
    "            }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh dear Indian folks, so diverse and grand,\n",
      "From the peaks of the Himalayas to the shores of sand.\n",
      "Your culture so rich, your spices so divine,\n",
      "In your presence, we all feel like we're on cloud nine.\n",
      "\n",
      "With traditions that date back centuries old,\n",
      "Your stories and legends are worth more than gold.\n",
      "Whether celebrating Holi or Diwali with glee,\n",
      "Your joy and spirit are something to see.\n",
      "\n",
      "From Bollywood dances to cricket matches intense,\n",
      "You sure know how to keep us all in suspense.\n",
      "Your food, oh your food, a feast for the senses,\n",
      "From butter chicken to dosas, there's no pretenses.\n",
      "\n",
      "So here's to you, oh Indian friends,\n",
      "May your laughter never end.\n",
      "With your warmth and charm, you light up the day,\n",
      "In your vibrant spirit, we'll always find our way.\n"
     ]
    }
   ],
   "source": [
    "print(text_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2 - Image Generation using DALL-E 3 (Text to Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A white Long tailed cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_response = client.images.generate(\n",
    "    model = \"dall-e-3\",\n",
    "    prompt = prompt,\n",
    "    size = '1024x1024',\n",
    "    quality = 'standard',\n",
    "    n=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = image_response.data[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://oaidalleapiprodscus.blob.core.windows.net/private/org-UjdkAgIvH3y8UGKn7Snwjk68/user-2TFVTG5sbaRnJdxODRQL163B/img-9R5oL7NHJ1gy3SIPJmqlmVLp.png?st=2024-03-10T13%3A09%3A26Z&se=2024-03-10T15%3A09%3A26Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2024-03-09T18%3A29%3A20Z&ske=2024-03-10T18%3A29%3A20Z&sks=b&skv=2021-08-06&sig=aTQwUPGe0kQEsyTIg9p/Pqe7Zs3B/9GY3KlGK4u7oWg%3D'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_token = keys.get(\"gemini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=gemini_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro\n",
      "models/gemini-pro-vision\n"
     ]
    }
   ],
   "source": [
    "for models in genai.list_models():\n",
    "    if \"generateContent\" in models.supported_generation_methods:\n",
    "        print(models.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 1: Text Generation using Gemini 1 Pro (Text to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = genai.GenerativeModel(\"models/gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = \"exam breaking bad funny memes\"\n",
    "agent_text = \"\"\"I want you to create a funny meme caption from the following inputs. \n",
    "Following the template is mandatory.\"\"\"\n",
    "\n",
    "#Predict Category from the prompt\n",
    "category = \"Hollywood\"\n",
    "top_padding = \"Yes\"\n",
    "#Predict subcategory from the matching text\n",
    "subcategory = \"Web Series\"\n",
    "\n",
    "#For each relevant meme template, get the mandatory format for the template + tags\n",
    "mandatory_format = \"When ...\"\n",
    "template_text = \"You're goddamn right.\"\n",
    "tags = \"Breaking Bad, Walter White\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_prompt = f\"\"\" {agent_text},\n",
    "User prompt: {input_prompt},\n",
    "Template Format: {mandatory_format},\n",
    "category - {category},\n",
    "subcategory - {subcategory}, \n",
    "tags: {tags} \n",
    "The caption generated should contain a top padding if top_padding is Yes where the text should be written in mandatory format provided. \n",
    "Top Padding: {top_padding} \n",
    "The text generated should make sense with the template text provided - {template_text}\n",
    "Once the meme caption is generated, check again if the caption with the template text is making sense.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_text_response = model_text.generate_content(meme_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top Padding: Yes**\n",
      "\n",
      "**When you ace your exam after spending the whole night binge-watching Breaking Bad**\n",
      "\n",
      "```\n",
      "You're goddamn right.\n",
      "```\n",
      "\n",
      "**Category:** Hollywood\n",
      "**Subcategory:** Web Series\n",
      "**Tags:** Breaking Bad, Walter White\n"
     ]
    }
   ],
   "source": [
    "print(gemini_text_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2: Image to Text using Gemini Pro Vision (Image to Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_image = genai.GenerativeModel('models/gemini-1.0-pro-vision-latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PIL.Image.open(\"youre-goddamn-right.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "image2text_response = model_image.generate_content(im).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When my wife asks if I farted.\n"
     ]
    }
   ],
   "source": [
    "print(image2text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When my wife asks if I farted.\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for sentence in image2text_response.split(\".\"):\n",
    "    print(sentence.strip() + \".\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
