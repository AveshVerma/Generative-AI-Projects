{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Basics Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Model Input/Output\n",
    "- Data Connectors\n",
    "- Chains\n",
    "- Memory\n",
    "- Agent -- To get data from 3rd party\n",
    "\n",
    "Topics\n",
    "1. Full Stack Application built by Langchain\n",
    "2. Prompting\n",
    "3. Chain of Prompts\n",
    "4. Tools, Initialization of Agent\n",
    "5. Custom Agent\n",
    "6. Token Limit Handling Strategies\n",
    "7. Deep Dive into Memory\n",
    "8. RAG System with Vector Databases\n",
    "9. LLM Application development Landscape\n",
    "10. LLM Application in Production\n",
    "11. Langserve\n",
    "12. Langsmith\n",
    "13. Text Splitting, Text Chunking\n",
    "14. LlamaIndex vs Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv() # Take Enviroment variables from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Can you tell me about the China?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "China, officially known as the People's Republic of China, is a country located in East Asia. It is the world's most populous country, with a population of over 1.4 billion people. It is also the third largest country by land area, spanning over 9.6 million square kilometers.\n",
      "\n",
      "The history of China can be traced back to over 5,000 years, with a series of dynasties ruling the country until the establishment of the Republic of China in 1912. In 1949, the Communist Party of China took control of the country and established the People's Republic of China.\n",
      "\n",
      "China is a socialist country with a single-party political system, and the government plays a significant role in the economy and society. In recent decades, China has undergone rapid economic growth, becoming the world's second-largest economy and a major player in global trade.\n",
      "\n",
      "The country is known for its rich cultural heritage, including its ancient architecture, art, literature, and philosophy. The Chinese language, Mandarin, is the most widely spoken language in the world.\n",
      "\n",
      "China has a diverse landscape, with mountains, plateaus, deserts, and coastlines, making it a popular tourist destination. It is also home to many famous landmarks, such as the Great Wall of\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/aveshverma/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "gemma_llm = HuggingFaceEndpoint(repo_id = \"google/gemma-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", New Delhi is a buzzing city of people, cars, bikes, buses, vendors, shops, temples, mosques and other things that would be difficult to describe. This capital city of India has its own unique charm and attraction for the tourists. From the UNESCO World Heritage site, Qutub Minar to the awesome Indian War Memorial, India Gate, to the rich Mughal architecture of Humayun’s Tomb and the modern architectures of the Parliament and Rashtrapati Bhavan, New Delhi is a city with a lot of things to see and do.\n",
      "\n",
      "<h2><strong>History of Delhi</strong></h2>\n",
      "\n",
      "Delhi has been known by several names in the past. The name Delhi is believed to be derived from Dilli or Dhillu, a king who ruled in the 6th century. Delhi is said to have been ruled by several dynasties in the past and has been known by several names such as Indraprastha, Lal Kot, Indrapat, Dhillikapuri, Yojakpuri, Mayapuri and many more. Delhi has seen its ups and downs and has been ruled by various foreign rulers like the Turks, Mongols, Mughals, British and the other Indian kingdoms.\n",
      "\n",
      "Delhi was declared as the capital of India by Lord Mountbatten and the city of New Delhi was designed by Lutyens. New Delhi is a planned city that was built keeping in mind the Mughal architecture of the past.\n",
      "\n",
      "<h2><strong>How to reach Delhi</strong></h2>\n",
      "\n",
      "Being the capital of India, Delhi is well connected by air, road and rail to most of the major cities in India as well as the major cities in the world.\n",
      "\n",
      "<h3><strong>Delhi by Air</strong></h3>\n",
      "\n",
      "The Indira Gandhi International Airport, New Delhi is one of the busiest airports in the world and is well connected with all the major cities in the world and all the major cities in India. There are flights available to and from all the major cities in India.\n",
      "\n",
      "<strong>Flights to Delhi from:</strong>\n",
      "\n",
      "* Mumbai to Delhi Flights\n",
      "* Bengaluru to Delhi Flights\n",
      "* Chennai to Delhi Flights\n",
      "* Hyderabad to Delhi Flights\n",
      "* Kolkata to Delhi Flights\n",
      "\n",
      "<h3><strong>Delhi by Road</strong></h3>\n",
      "\n",
      "Delhi is connected by road to all the major cities in India. There are buses and private taxis available from all the major cities in India.\n",
      "\n",
      "<h3><strong>Delhi by Rail</strong></h3>\n",
      "\n",
      "New Delhi Railway Station is the main station in Delhi that is well connected by rail to all the major cities in India.\n",
      "\n",
      "<h2><strong>Best time to visit Delhi</strong></h2>\n",
      "\n",
      "The best time\n"
     ]
    }
   ],
   "source": [
    "print(gemma_llm.invoke(\"Capital of India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = HuggingFaceHub(repo_id=\"google/flan-t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you are a slob'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"How are you in today's time?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm4=ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mai marr bhi jata na magar, category - Bollywood, subcategory - Paresh Rawal, tags: Welcome movie\n",
      "**Caption:** \"Aankh maare, tujhe maare, jo na maare ohi mare!\"\n",
      "- Paresh Rawal, Welcome\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I want you to create a funny meme caption from the following inputs. Following the template is mandatory. Something safely funny. Template Format: Mai marr bhi jata na magar, category - Bollywood, subcategory - Paresh Rawal, tags: Welcome movie\"\n",
    "gemini_response = llm4.invoke(prompt).content\n",
    "print(gemini_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Complete Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- temperature - How creative we want our model to be\n",
    "\n",
    "0 -> Safe, not taking any bets \n",
    "1 -> Model will take risk to be creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good name of a company that sells colorful socks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rainbow Threads Co.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Templatte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prompt_template_name.format(cuisine = \"German\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for German food. Suggest a fancy name for this'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt)\n",
    "prompt.format(cuisine = \"German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Kaiser's Table\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "response = chain.invoke(\"German\")\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.6)\n",
    "prompt1 = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this\"\n",
    "prompt2 = \"Suggest some good menu items for {restaurant_name}\"\n",
    "\n",
    "\n",
    "prompt_template1 = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = prompt1\n",
    ")\n",
    "\n",
    "prompt_template2 = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = prompt2\n",
    ")\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template1)\n",
    "item_chain = LLMChain(llm = llm, prompt = prompt_template2)\n",
    "\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain, item_chain])\n",
    "response = chain.invoke([\"Indian\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Chicken Tikka Masala: Tender chicken marinated in spices and cooked in a creamy tomato sauce.\n",
      "2. Vegetable Samosas: Crispy fried pastries filled with spiced potatoes, peas, and onions.\n",
      "3. Lamb Vindaloo: A spicy and tangy dish made with tender lamb, vinegar, and a variety of spices.\n",
      "4. Palak Paneer: Fresh spinach cooked with cubes of paneer cheese in a flavorful gravy.\n",
      "5. Tandoori Chicken: Chicken marinated in yogurt and spices, cooked in a clay oven for a smoky flavor.\n",
      "6. Chana Masala: A popular vegetarian dish made with chickpeas, onions, tomatoes, and aromatic spices.\n",
      "7. Butter Naan: Soft and fluffy Indian bread brushed with butter and cooked in a tandoor oven.\n",
      "8. Biryani: Fragrant basmati rice cooked with a choice of chicken, lamb, or vegetables and a blend of spices.\n",
      "9. Malai Kofta: Vegetable and cheese dumplings served in a rich and creamy tomato-based gravy.\n",
      "10. Mango Lassi: A refreshing drink made with yogurt, mango puree, and a touch of cardamom.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the intermediate output as well - we use SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature = 0.6)\n",
    "prompt1 = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this\"\n",
    "prompt2 = \"Suggest some good menu items for {restaurant_name}\"\n",
    "out1_key = \"restaurant_name\"\n",
    "out2_key = \"menu_items\"\n",
    "\n",
    "\n",
    "prompt_template1 = PromptTemplate(\n",
    "    input_variables = ['cuisine'],\n",
    "    template = prompt1\n",
    ")\n",
    "\n",
    "prompt_template2 = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template = prompt2\n",
    ")\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template1, output_key = out1_key)\n",
    "item_chain = LLMChain(llm = llm, prompt = prompt_template2, output_key = out2_key)\n",
    "\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, item_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = [out1_key, out2_key]\n",
    ")\n",
    "response = chain.invoke([\"Indian\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To fine tune LLMs online with custom data, use RAG Collection in Gradient AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents with SERP API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to search data from the Google and use LLM simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "SERPAPI_API_KEY=os.getenv(\"SERPAPI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the current GDP of the US and then use a calculator to project the GDP for 2023.\n",
      "Action: Search\n",
      "Action Input: \"US GDP 2021\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"The U.S. is a country of 50 states covering a vast swath of North America, with Alaska in the northwest and Hawaii extending the nation’s presence into the Pacific Ocean. Major Atlantic Coast cities are New York, a global finance and culture center, and capital Washington, DC. Midwestern metropolis Chicago is known for influential architecture and on the west coast, Los Angeles' Hollywood is famed for filmmaking. ― Google\", 'United States type: Country in North America.', 'United States kgmid: /m/09c7w0.', 'Real gross domestic product (GDP) increased at an annual rate of 7.0 percent in the fourth quarter of 2021 (table 1), according to the \"second\" ...', 'Virgin Islands (U.S.). 2021. 4,444.00. West Bank and Gaza. 2022. 19,111.90. Yemen, Rep. 2018. 21,606.16. Zambia. 2022. 29,163.78. Zimbabwe. 2022. 27,366.63.', 'Real gross domestic product (GDP) increased at an annual rate of 3.2 percent in the fourth quarter of 2023, according to the “second” estimate.', 'U.S. gdp for 2021 was $23,315.08B, a 10.71% increase from 2020. U.S. gdp for 2020 was $21,060.47B, a 1.5% decline from 2019. U.S. gdp for 2019 was $21,380.98B, ...', 'View economic output, reported as the nominal value of all new goods and services produced by labor and property located in the U.S..', 'GDP in the United States is expected to reach 27780.00 USD Billion by the end of 2024, according to Trading Economics global macro models and analysts ...', 'In 2023 the real gross domestic product (GDP) of the United States increased by 2.5 percent compared to 2022. This rate of annual growth ...', 'Measured from the fourth quarter of 2022 to the fourth quarter of 2023, real GDP increased 3.1 percent during the period compared with an ...', 'Historical Data ; March 31, 2021, 5.20% ; December 31, 2020, 4.20% ; September 30, 2020, 34.80% ; June 30, 2020, -28.00%.', \"Gross domestic product accelerated to a 4.9% annualized rate, more than double the second-quarter pace, according to the government's ...\"]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now I can use a calculator to project the GDP for 2023.\n",
      "Action: Calculator\n",
      "Action Input: 3.1% growth rate\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 0.031\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The projected GDP for the US in 2023 is $27,366.63 billion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The projected GDP for the US in 2023 is $27,366.63 billion.'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature = 0)\n",
    "\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"What is the GDP of US in 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (2010 film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (2010 film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release date of Inception\n",
      "Action: Wikipedia\n",
      "Action Input: Inception (2010 film)\u001b[0m\n",
      "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use Calculator to find the release date of Inception\n",
      "Action: Calculator\n",
      "Action Input: 2019-2010\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 9\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The release date of Inception was 2010.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The release date of Inception was 2010.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['wikipedia', 'llm-math'], llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "agent.run(\"When did Inception release?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Kaiser's Kitchen\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt)\n",
    "prompt.format(cuisine = \"German\")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt)\n",
    "response = chain.invoke(\"German\")\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Maharaja's Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "response2 = chain.invoke(\"Indian\")\n",
    "print(response2[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here since we are not using memory to store the information that it generated, it is returning as NoneType.\n",
    "To use memory we use ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Kaiser's Kitchen\"\n",
      "\n",
      "\n",
      "\"Maharaja's Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm = llm, prompt = prompt, memory = memory)\n",
    "response = chain.invoke(\"German\")\n",
    "print(response[\"text\"])\n",
    "response2 = chain.invoke(\"Indian\")\n",
    "print(response2[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: German\n",
      "AI: \n",
      "\n",
      "\"Kaiser's Kitchen\"\n",
      "Human: Indian\n",
      "AI: \n",
      "\n",
      "\"Maharaja's Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will remember all the output in its memory. This leads to high memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The first Cricket World Cup was held in 1975 and was won by the West Indies team. The final match was played between West Indies and Australia at Lord's Cricket Ground in London. West Indies won by 17 runs, with Clive Lloyd scoring a century. It was a historic moment for the West Indies team as it was their first time winning the World Cup.\""
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "\n",
    "convo = ConversationChain(llm = OpenAI(temperature = 0.7))\n",
    "\n",
    "print(convo.prompt.template)\n",
    "\n",
    "convo.run(\"Who won the first Cricket World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The answer to 5+5 is 10.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 5+5?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  As I mentioned earlier, the captain of the winning team was Clive Lloyd. He was a left-handed batsman and an all-rounder for the West Indies team. He also holds the record for most runs scored in a World Cup final, with 102 runs in the 1975 final.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is remembering the previous responses and can catch up with the relevant information even after a different context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Buffer Window Memory - to save last k prompt responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm = OpenAI(temperature = 0.7), memory = memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first Cricket World Cup was held in 1975 in England and was won by the West Indies team. They beat Australia by 17 runs in the final match.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first Cricket World Cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5+5 is 10. It is a simple addition problem and the answer is a basic fact in mathematics. Do you have any other questions?'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"What is 5+5?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I do not have enough context to accurately answer that question. Can you provide me with more information?'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since k=1, only last response is being currently saved by the Memory module. Hence it is saying that it does not have context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"sample_data.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages) # Number of pages in PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
