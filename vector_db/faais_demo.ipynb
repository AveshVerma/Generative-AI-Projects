{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS - VectorDB Developed by Facebook Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like ChromaDB, FAISS is a Local Vector DB, unlike Pinecone that is a cloud VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install langchain\n",
    "# !pip install pypdf\n",
    "# !pip install sentence-transformers==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install tiktoken\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "much ear wax. \n",
      "These eclectic exercises will help you build a flexible and \n",
      "robust framework for working through a wide range of \n",
      "challenges, from truly grokking current events to handling \n",
      "the daily surprises of the business world. \n",
      "You‘ll learn how to:\n",
      "• Calculate distributions to see the range of your beliefs\n",
      "• Compare hypotheses and draw reliable conclusions • Calculate Bayes’ theorem and understand what it’s \n",
      "useful for\n",
      "• Find the posterior, likelihood, and prior to check the\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Embedding from the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Load the VectorDB to store the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Bayesian model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bayesian statistics the fun Way\n",
      "One particularly nice thing about Bayesian statistics is that, because we \n",
      "can view it simply as reasoning about uncertain things, all of the tools and techniques of Bayesian statistics make intuitive sense.\n",
      "Bayesian statistics is about looking at a problem you face, figuring out\n",
      "1\n",
      "bayesian t hinkinG  an D \n",
      "eve RyDay Reas Onin G\n",
      "In this first chapter, I’ll give you an over -\n",
      "view of Bayesian reasoning , the formal pro -\n",
      "cess we use to update our beliefs about the \n",
      "world once we’ve observed some data. We’ll \n",
      "work through a scenario and explore how we can map \n",
      "our everyday experience to Bayesian reasoning. \n",
      "The good news is that you were already a Bayesian even before you \n",
      "picked up this book! Bayesian statistics is closely aligned with how people\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # Take Enviroment variables from .env\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, chain_type='stuff', retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bayesian model is a hypothesis or belief about how the world works that makes predictions based on the observed data. It is a key component of Bayesian statistics, which is a method of reasoning about uncertain things.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Bayesian model?\"\n",
    "\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
